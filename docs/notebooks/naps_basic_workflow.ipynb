{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__EV0njV4R2-"
      },
      "source": [
        "# Example NAPS usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlV70jDuWzea"
      },
      "source": [
        "In this notebook we'll install NAPS, pull the example data from the [GitHub repository](https://github.com/kocherlab/naps), and run `naps-track` against it.\n",
        "\n",
        "This repo is particularly useful in combination with SLEAP's example notebook on remote training and inference which can be found [here](https://colab.research.google.com/github/talmolab/sleap/blob/main/docs/notebooks/Training_and_inference_on_an_example_dataset.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX9noEb8m8re"
      },
      "source": [
        "## Install NAPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUfnkxMtLcK3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q naps-track"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2imnIw6QWa1z"
      },
      "outputs": [],
      "source": [
        "# # If you have a model and want to do the inference on Colab, this can be done quite directly! Just upload your model and run inference as below.\n",
        "# # You can also take advantage of the GPU accessibility of Colab to train as well. Look to the SLEAP tutorials for more info.\n",
        "# sleap-track example.mp4 -o \"example.slp\" -m models/bu --verbosity json --batch_size 4 --verbosity json --tracking.tracker simple --tracking.similarity iou --tracker.track_window 5 --tracking.post_connect_single_breaks 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq7jrgUksLtR"
      },
      "source": [
        "## Download sample training data into Colab\n",
        "Let's download a sample dataset from the the NAPS repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBUTQ2Cm44En"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://github.com/kocherlab/naps/raw/main/tests/data/example.slp\n",
        "!wget https://github.com/kocherlab/naps/raw/main/tests/data/example.analysis.h5\n",
        "!wget https://github.com/kocherlab/naps/raw/main/tests/data/example.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaiFu3s3HVcH"
      },
      "outputs": [],
      "source": [
        "!ls -lht"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIsKUX661xFK"
      },
      "source": [
        "## NAPS tracking\n",
        "Now let's track the files using `naps-track`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi4ksVdCKbBc"
      },
      "outputs": [],
      "source": [
        "!naps-track --slp-path example.slp --h5-path example.analysis.h5 --video-path example.mp4 --tag-node 0 \\\n",
        " --start-frame 0 --end-frame 1203 --aruco-marker-set DICT_4X4_100 \\\n",
        " --output-path example_output.analysis.h5 --aruco-error-correction-rate 0 \\\n",
        " --aruco-adaptive-thresh-constant 12 --aruco-adaptive-thresh-win-size-max 30 \\\n",
        " --aruco-adaptive-thresh-win-size-step 3 --aruco-perspective-rm-ignored-margin 0.13 \\\n",
        " --aruco-adaptive-thresh-win-size-min 3 --half-rolling-window-size 11 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXDLsB7tKRZ_"
      },
      "source": [
        "## Download\n",
        "\n",
        "Now we can just download the output! This pulls the video, the output file, and the original project. \n",
        "\n",
        "Once you have these, you can create a new SLEAP project and import example_output.analysis.h5 and point to the video to see the resulting tracks. If you are curious how these compare with the original tracks, you can open the original project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej2it8dl_BO_"
      },
      "outputs": [],
      "source": [
        "# # Zip the video and output\n",
        "# !zip -0 -r naps_output.zip example.mp4 example.slp example_output.analysis.h5\n",
        "\n",
        "# # Download\n",
        "# from google.colab import files\n",
        "# files.download(\"/content/naps_output.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fy26NVmCWFw"
      },
      "source": [
        "If you happen to not be using Chrome, you may get an error here. If that happens, you should be able to download the files using the \"Files\" tab on the left side panel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERsPYhzbx4GR"
      },
      "source": [
        "## After NAPS\n",
        "\n",
        "### SLEAP GUI\n",
        "\n",
        "Now that we've got the files, we can either open the raw data or import it back into SLEAP. To view the tracks, open SLEAP (`sleap-label`) and create a new project. After creating a new project, you can go File > Import > SLEAP Analysis HDF5 and select the output, here example_output.analysis.h5. When you select the file, you will be prompted to select the video associated with the analysis file. You can simply select example.mp4 and then tracks will display in SLEAP.\n",
        "\n",
        "### Directly reading the output H5\n",
        "\n",
        "Now let's try reading in the .h5 directly and plotting a couple of basic features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmlycvykyc_2"
      },
      "outputs": [],
      "source": [
        "\"\"\"Read in the h5 and display basic info\n",
        "\"\"\"\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "filename = \"example_output.analysis.h5\"\n",
        "video_filename = \"example.mp4\"\n",
        "output_filename = \"output.mp4\"\n",
        "\n",
        "with h5py.File(filename, \"r\") as f:\n",
        "    dset_names = list(f.keys())\n",
        "    locations = f[\"tracks\"][:].T\n",
        "    node_names = [n.decode() for n in f[\"node_names\"][:]]\n",
        "\n",
        "print(\"===filename===\")\n",
        "print(filename)\n",
        "print()\n",
        "\n",
        "print(\"===HDF5 datasets===\")\n",
        "print(dset_names)\n",
        "print()\n",
        "\n",
        "print(\"===locations data shape===\")\n",
        "print(locations.shape)\n",
        "print()\n",
        "\n",
        "print(\"===nodes===\")\n",
        "for i, name in enumerate(node_names):\n",
        "    print(f\"{i}: {name}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PvF1zw0HuxM"
      },
      "source": [
        "### Utility functions for cleaning up tracks, plotting, and showing the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuLrL9D0zadQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"Resource functions\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import scipy.ndimage\n",
        "from tqdm import tqdm\n",
        "from scipy.signal import savgol_filter\n",
        "import matplotlib.colors as colors\n",
        "import logging\n",
        "import skvideo.io\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import palettable\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "def flatten_features(x, axis=0):\n",
        "\n",
        "    if axis != 0:\n",
        "        # Move time axis to the first dim\n",
        "        x = np.moveaxis(x, axis, 0)\n",
        "\n",
        "    # Flatten to 2D.\n",
        "    initial_shape = x.shape\n",
        "    x = x.reshape(len(x), -1)\n",
        "\n",
        "    return x, initial_shape\n",
        "\n",
        "\n",
        "def unflatten_features(x, initial_shape, axis=0):\n",
        "    # Reshape.\n",
        "    x = x.reshape(initial_shape)\n",
        "\n",
        "    if axis != 0:\n",
        "        # Move time axis back\n",
        "        x = np.moveaxis(x, 0, axis)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def smooth_median(x, window=5, axis=0, inplace=False):\n",
        "    if axis != 0 or x.ndim > 1:\n",
        "        if not inplace:\n",
        "            x = x.copy()\n",
        "\n",
        "        # Reshape to (time, D)\n",
        "        x, initial_shape = flatten_features(x, axis=axis)\n",
        "\n",
        "        # Apply function to each slice\n",
        "        for i in range(x.shape[1]):\n",
        "            x[:, i] = smooth_median(x[:, i], window, axis=0)\n",
        "\n",
        "        # Restore to original shape\n",
        "        x = unflatten_features(x, initial_shape, axis=axis)\n",
        "        return x\n",
        "\n",
        "    y = scipy.signal.medfilt(x.copy(), window)\n",
        "    y = y.reshape(x.shape)\n",
        "    mask = np.isnan(y) & (~np.isnan(x))\n",
        "    y[mask] = x[mask]\n",
        "    return y\n",
        "\n",
        "\n",
        "def fill_missing(x, kind=\"nearest\", axis=0, **kwargs):\n",
        "    \"\"\"Fill missing values in a timeseries.\n",
        "    Args:\n",
        "        x: Timeseries of shape (time, ...) or with time axis specified by axis.\n",
        "        kind: Type of interpolation to use. Defaults to \"nearest\".\n",
        "        axis: Time axis (default: 0).\n",
        "    Returns:\n",
        "        Timeseries of the same shape as the input with NaNs filled in.\n",
        "    Notes:\n",
        "        This uses pandas.DataFrame.interpolate and accepts the same kwargs.\n",
        "    \"\"\"\n",
        "    if x.ndim > 2:\n",
        "        # Reshape to (time, D)\n",
        "        x, initial_shape = flatten_features(x, axis=axis)\n",
        "\n",
        "        # Interpolate.\n",
        "        x = fill_missing(x, kind=kind, axis=0, **kwargs)\n",
        "\n",
        "        # Restore to original shape\n",
        "        x = unflatten_features(x, initial_shape, axis=axis)\n",
        "\n",
        "        return x\n",
        "\n",
        "    return pd.DataFrame(x).interpolate(method=kind, axis=axis, **kwargs).to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "def plot_trx(\n",
        "    tracks,\n",
        "    video_path=None,\n",
        "    shift=0,\n",
        "    frame_start=0,\n",
        "    frame_end=100,\n",
        "    trail_length=10,\n",
        "    output_path=\"output.mp4\",\n",
        "    color_map=None,\n",
        "    id_map=None,\n",
        "    scale_factor=1,\n",
        "    annotate=False,\n",
        "):\n",
        "    ffmpeg_writer = skvideo.io.FFmpegWriter(\n",
        "        f\"{output_path}\", inputdict={'-r':\"20\"}, outputdict={\"-vcodec\": \"libx264\"}\n",
        "    )\n",
        "    if video_path != None:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_start - 1)\n",
        "    data = tracks[(frame_start+shift):(frame_end+shift), :, :, :]\n",
        "    dpi = 300\n",
        "    for frame_idx in tqdm(range(data.shape[0]), position=0, leave=True):\n",
        "        fig, ax = plt.subplots(figsize=(3660 / dpi, 3660 / dpi), dpi=dpi)\n",
        "        plt.gca().invert_yaxis()\n",
        "        # plt.xlim((0, 3660))\n",
        "        # plt.ylim((-3660, 0))\n",
        "        data_subset = data[max((frame_idx - trail_length), 0) : frame_idx, :, :, :]\n",
        "        for fly_idx in range(data_subset.shape[3]):\n",
        "            if annotate and data_subset.shape[0] > 0:\n",
        "                # if  ~(data_subset[-1, 0, 1, fly_idx] < 3660/2):\n",
        "                plt.annotate(\n",
        "                    fly_idx,\n",
        "                    (data_subset[-1, 0, 0, fly_idx], data_subset[-1, 0, 1, fly_idx]),\n",
        "                    size=18,\n",
        "                    ha=\"left\",\n",
        "                    va=\"bottom\",\n",
        "                    color=\"#CB9E23\",\n",
        "                )\n",
        "            for node_idx in range(data_subset.shape[1]):\n",
        "                for idx in range(2, data_subset.shape[0]):\n",
        "                    # if  data_subset[idx, node_idx, 1, fly_idx] < 3660/2:\n",
        "                    #     continue\n",
        "                    # Note that you need to use single steps or the data has \"steps\"\n",
        "                    if color_map == None:\n",
        "                        plt.plot(\n",
        "                            data_subset[(idx - 2) : idx, node_idx, 0, fly_idx],\n",
        "                            data_subset[(idx - 2) : idx, node_idx, 1, fly_idx],\n",
        "                            linewidth= 4.5 * idx / data_subset.shape[0],\n",
        "                            color=palettable.tableau.Tableau_20.mpl_colors[node_idx],\n",
        "                        )\n",
        "                    else:\n",
        "                        color = color_map[id_map[fly_idx]]\n",
        "                        (l,) = ax.plot(\n",
        "                            data_subset[(idx - 2) : idx, node_idx, 0, fly_idx]\n",
        "                            * scale_factor,\n",
        "                            data_subset[(idx - 2) : idx, node_idx, 1, fly_idx]\n",
        "                            * scale_factor,\n",
        "                            linewidth=3 * idx / data_subset.shape[0],\n",
        "                            color=color,\n",
        "                        )\n",
        "                        l.set_solid_capstyle(\"round\")\n",
        "        if video_path != None:\n",
        "            if cap.isOpened():\n",
        "                res, frame = cap.read()\n",
        "                frame = frame[:, :, 0]\n",
        "                # frame[:,:,:] = 255\n",
        "                plt.imshow(frame, cmap=\"gray\")\n",
        "        ax = plt.Axes(fig, [0.0, 0.0, 1.0, 1.0])\n",
        "        ax.set_axis_off()\n",
        "        fig.add_axes(ax)\n",
        "        fig.set_size_inches(3660 / dpi, 3660 / dpi, True)\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "        ax.axis(\"off\")\n",
        "        fig.patch.set_visible(False)\n",
        "        fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None)\n",
        "        fig.canvas.draw()\n",
        "        image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "        image_from_plot = image_from_plot.reshape(\n",
        "            fig.canvas.get_width_height()[::-1] + (3,)\n",
        "        )\n",
        "        ffmpeg_writer.writeFrame(image_from_plot)\n",
        "        plt.close()\n",
        "    ffmpeg_writer.close()\n",
        "\n",
        "def instance_node_velocities(fly_node_locations, start_frame, end_frame):\n",
        "    frame_count = len(range(start_frame, end_frame))\n",
        "    if len(fly_node_locations.shape) == 4:\n",
        "        fly_node_velocities = np.zeros(\n",
        "            (frame_count, fly_node_locations.shape[1], fly_node_locations.shape[3])\n",
        "        )\n",
        "        for fly_idx in range(fly_node_locations.shape[3]):\n",
        "            for n in tqdm(range(0, fly_node_locations.shape[1])):\n",
        "                fly_node_velocities[:, n, fly_idx] = diff(\n",
        "                    fly_node_locations[start_frame:end_frame, n, :, fly_idx]\n",
        "                )\n",
        "    else:\n",
        "        fly_node_velocities = np.zeros((frame_count, fly_node_locations.shape[1]))\n",
        "        for n in tqdm(range(0, fly_node_locations.shape[1] - 1)):\n",
        "            fly_node_velocities[:, n] = diff(\n",
        "                fly_node_locations[start_frame:end_frame, n, :]\n",
        "            )\n",
        "\n",
        "    return fly_node_velocities\n",
        "\n",
        "\n",
        "def diff(node_loc, diff_func = np.gradient, **kwargs):\n",
        "    \"\"\"\n",
        "    node_loc is a [frames, 2] arrayF\n",
        "\n",
        "    win defines the window to smooth over\n",
        "\n",
        "    poly defines the order of the polynomial\n",
        "    to fit with\n",
        "\n",
        "    \"\"\"\n",
        "    node_loc_vel = np.zeros_like(node_loc)\n",
        "    for c in range(node_loc.shape[-1]):\n",
        "        node_loc_vel[:, c] = diff_func(node_loc[:, c], **kwargs)\n",
        "\n",
        "    node_vel = np.linalg.norm(node_loc_vel,axis=1)\n",
        "\n",
        "    return node_vel\n",
        "\n",
        "def show_video(video_path, video_width = 1000):\n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        "\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYaB5dugSx4b"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "px_mm = 15.5\n",
        "\n",
        "# Missingness filter\n",
        "atleast_one_node_defined = np.any(~np.isnan(locations[:, :, 0, :]), axis=1)\n",
        "no_nodes_defined =  ~atleast_one_node_defined\n",
        "\n",
        "missing_ct = np.sum(no_nodes_defined, axis=0)\n",
        "missing_freq = missing_ct / no_nodes_defined.shape[0]\n",
        "locations_filtered = locations[:, : , :, missing_freq < 0.8]\n",
        "\n",
        "\n",
        "vel = instance_node_velocities(locations_filtered,0,locations_filtered.shape[0])\n",
        "mask_2d = ~(vel[:,node_names.index('Thorax'),:] < px_mm*5 )[:,np.newaxis,np.newaxis,:]\n",
        "mask_4d = np.broadcast_to(mask_2d, locations_filtered.shape)\n",
        "locations_filtered[mask_4d] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OpYyGTM0c1O"
      },
      "outputs": [],
      "source": [
        "# Write a video!\n",
        "plot_trx(locations_filtered, video_path = video_filename, output_path = output_filename, frame_start = 0, frame_end=100, trail_length=5, annotate=True, shift = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n598v3Eu2Q0g"
      },
      "outputs": [],
      "source": [
        "# Now lets do a little Jupyter magic to display the video in browser! \n",
        "show_video(output_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLWnV8Qz6drk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "naps_basic_workflow",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.11 ('sleap_dev')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "177c836934b6a63d57a075b5cc3f7812a3de8a98495a556647184ecb20fdf5fb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
