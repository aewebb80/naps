{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDq-1wfhRBWs"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kocherlab/naps/blob/main/docs/notebooks/naps_graphics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__EV0njV4R2-"
      },
      "source": [
        "# Example NAPS graphics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX9noEb8m8re"
      },
      "source": [
        "## Install NAPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUfnkxMtLcK3"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install -q naps-track"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2imnIw6QWa1z"
      },
      "outputs": [],
      "source": [
        "# # If you have a model and want to do the inference on Colab, this can be done quite directly! Just upload your model and run inference as below.\n",
        "# # You can also take advantage of the GPU accessibility of Colab to train as well. Look to the SLEAP tutorials for more info.\n",
        "# sleap-track example.mp4 -o \"example.slp\" -m models/bu --verbosity json --batch_size 4 --verbosity json --tracking.tracker simple --tracking.similarity iou --tracker.track_window 5 --tracking.post_connect_single_breaks 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq7jrgUksLtR"
      },
      "source": [
        "## Download sample training data into Colab\n",
        "Let's download a sample dataset from the the NAPS repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBUTQ2Cm44En"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !wget https://github.com/kocherlab/naps/raw/main/tests/data/example.slp\n",
        "# !wget https://github.com/kocherlab/naps/raw/main/tests/data/example.analysis.h5\n",
        "# !wget https://github.com/kocherlab/naps/raw/main/tests/data/example.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaiFu3s3HVcH"
      },
      "outputs": [],
      "source": [
        "!ls -lht"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIsKUX661xFK"
      },
      "source": [
        "## NAPS tracking\n",
        "Now let's track the files using `naps-track`. We've adjusted a couple params here to make the tracks nicer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bi4ksVdCKbBc"
      },
      "outputs": [],
      "source": [
        "# !naps-track --slp-path example.slp --h5-path example.analysis.h5 --video-path example.mp4 --tag-node 0 \\\n",
        "#  --start-frame 0 --end-frame 1199 --aruco-marker-set DICT_4X4_100 \\\n",
        "#  --output-path example_output.analysis.h5 --aruco-error-correction-rate 0.6 \\\n",
        "#  --aruco-adaptive-thresh-constant 3 --aruco-adaptive-thresh-win-size-max 50 \\\n",
        "#  --aruco-adaptive-thresh-win-size-step 12 --aruco-perspective-rm-ignored-margin 0.33 \\\n",
        "#  --aruco-adaptive-thresh-win-size-min 3 --half-rolling-window-size 21 \n",
        "\n",
        "# One liner:\n",
        "# naps-track --slp-path example_1h_1130to1230pm.slp --h5-path example_1h_1130to1230pm.analysis.h5 --video-path example_1h_1130to1230pm.mp4 --tag-node 0 --start-frame 0 --end-frame 1200 --aruco-marker-set DICT_4X4_100 --output-path example_1h_1130to1230pm_naps.analysis.h5 --aruco-error-correction-rate 0.6 --aruco-adaptive-thresh-constant 7 --aruco-adaptive-thresh-win-size-max 23 --aruco-adaptive-thresh-win-size-step 10 --aruco-perspective-rm-ignored-margin 0.33 --aruco-adaptive-thresh-win-size-min 3 --half-rolling-window-size 21"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERsPYhzbx4GR"
      },
      "source": [
        "## After NAPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmlycvykyc_2"
      },
      "outputs": [],
      "source": [
        "\"\"\"Read in the h5 and display basic info\n",
        "\"\"\"\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "filename = \"example_output_1h.analysis.h5\"\n",
        "video_filename = \"example_1h.mp4\"\n",
        "output_filename = \"output_1h.mp4\"\n",
        "\n",
        "with h5py.File(filename, \"r\") as f:\n",
        "    dset_names = list(f.keys())\n",
        "    locations = f[\"tracks\"][:].T\n",
        "    node_names = [n.decode() for n in f[\"node_names\"][:]]\n",
        "\n",
        "print(\"===filename===\")\n",
        "print(filename)\n",
        "print()\n",
        "\n",
        "print(\"===HDF5 datasets===\")\n",
        "print(dset_names)\n",
        "print()\n",
        "\n",
        "print(\"===locations data shape===\")\n",
        "print(locations.shape)\n",
        "print()\n",
        "\n",
        "print(\"===nodes===\")\n",
        "for i, name in enumerate(node_names):\n",
        "    print(f\"{i}: {name}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PvF1zw0HuxM"
      },
      "source": [
        "### Utility functions for cleaning up tracks, plotting, and showing the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuLrL9D0zadQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"Resource functions\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import scipy.ndimage\n",
        "from tqdm import tqdm\n",
        "from scipy.signal import savgol_filter\n",
        "import matplotlib.colors as colors\n",
        "import logging\n",
        "import skvideo.io\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import palettable\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "def flatten_features(x, axis=0):\n",
        "\n",
        "    if axis != 0:\n",
        "        # Move time axis to the first dim\n",
        "        x = np.moveaxis(x, axis, 0)\n",
        "\n",
        "    # Flatten to 2D.\n",
        "    initial_shape = x.shape\n",
        "    x = x.reshape(len(x), -1)\n",
        "\n",
        "    return x, initial_shape\n",
        "\n",
        "\n",
        "def unflatten_features(x, initial_shape, axis=0):\n",
        "    # Reshape.\n",
        "    x = x.reshape(initial_shape)\n",
        "\n",
        "    if axis != 0:\n",
        "        # Move time axis back\n",
        "        x = np.moveaxis(x, 0, axis)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def smooth_median(x, window=5, axis=0, inplace=False):\n",
        "    if axis != 0 or x.ndim > 1:\n",
        "        if not inplace:\n",
        "            x = x.copy()\n",
        "\n",
        "        # Reshape to (time, D)\n",
        "        x, initial_shape = flatten_features(x, axis=axis)\n",
        "\n",
        "        # Apply function to each slice\n",
        "        for i in range(x.shape[1]):\n",
        "            x[:, i] = smooth_median(x[:, i], window, axis=0)\n",
        "\n",
        "        # Restore to original shape\n",
        "        x = unflatten_features(x, initial_shape, axis=axis)\n",
        "        return x\n",
        "\n",
        "    y = scipy.signal.medfilt(x.copy(), window)\n",
        "    y = y.reshape(x.shape)\n",
        "    mask = np.isnan(y) & (~np.isnan(x))\n",
        "    y[mask] = x[mask]\n",
        "    return y\n",
        "\n",
        "\n",
        "def fill_missing(x, kind=\"nearest\", axis=0, **kwargs):\n",
        "    \"\"\"Fill missing values in a timeseries.\n",
        "    Args:\n",
        "        x: Timeseries of shape (time, ...) or with time axis specified by axis.\n",
        "        kind: Type of interpolation to use. Defaults to \"nearest\".\n",
        "        axis: Time axis (default: 0).\n",
        "    Returns:\n",
        "        Timeseries of the same shape as the input with NaNs filled in.\n",
        "    Notes:\n",
        "        This uses pandas.DataFrame.interpolate and accepts the same kwargs.\n",
        "    \"\"\"\n",
        "    if x.ndim > 2:\n",
        "        # Reshape to (time, D)\n",
        "        x, initial_shape = flatten_features(x, axis=axis)\n",
        "\n",
        "        # Interpolate.\n",
        "        x = fill_missing(x, kind=kind, axis=0, **kwargs)\n",
        "\n",
        "        # Restore to original shape\n",
        "        x = unflatten_features(x, initial_shape, axis=axis)\n",
        "\n",
        "        return x\n",
        "\n",
        "    return pd.DataFrame(x).interpolate(method=kind, axis=axis, **kwargs).to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "def plot_trx(\n",
        "    tracks,\n",
        "    video_path=None,\n",
        "    shift=0,\n",
        "    frame_start=0,\n",
        "    frame_end=100,\n",
        "    trail_length=10,\n",
        "    output_path=\"output.mp4\",\n",
        "    color_map=None,\n",
        "    id_map=None,\n",
        "    scale_factor=1,\n",
        "    annotate=False,\n",
        "):\n",
        "    ffmpeg_writer = skvideo.io.FFmpegWriter(\n",
        "        f\"{output_path}\", inputdict={'-r':\"20\"}, outputdict={\"-vcodec\": \"libx264\"}\n",
        "    )\n",
        "    if video_path != None:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_start - 1)\n",
        "    data = tracks[(frame_start+shift):(frame_end+shift), :, :, :]\n",
        "    dpi = 300\n",
        "    for frame_idx in tqdm(range(data.shape[0]), position=0, leave=True):\n",
        "        fig, ax = plt.subplots(figsize=(1000 / dpi, 1000 / dpi), dpi=dpi)\n",
        "        plt.gca().invert_yaxis()\n",
        "        # plt.xlim((0, 3660))\n",
        "        # plt.ylim((-3660, 0))\n",
        "        data_subset = data[max((frame_idx - trail_length), 0) : frame_idx, :, :, :]\n",
        "        for fly_idx in range(data_subset.shape[3]):\n",
        "            if annotate and data_subset.shape[0] > 0:\n",
        "                # if  ~(data_subset[-1, 0, 1, fly_idx] < 3660/2):\n",
        "                plt.annotate(\n",
        "                    fly_idx,\n",
        "                    (data_subset[-1, 0, 0, fly_idx], data_subset[-1, 0, 1, fly_idx]),\n",
        "                    size=18,\n",
        "                    ha=\"left\",\n",
        "                    va=\"bottom\",\n",
        "                    color=\"#CB9E23\",\n",
        "                )\n",
        "            for node_idx in range(data_subset.shape[1]):\n",
        "                for idx in range(2, data_subset.shape[0]):\n",
        "                    if color_map == None:\n",
        "                        plt.plot(\n",
        "                            data_subset[(idx - 2) : idx, node_idx, 0, fly_idx],\n",
        "                            data_subset[(idx - 2) : idx, node_idx, 1, fly_idx],\n",
        "                            linewidth= 4.5 * idx / data_subset.shape[0],\n",
        "                            color=palettable.tableau.Tableau_20.mpl_colors[node_idx],\n",
        "                        )\n",
        "                    else:\n",
        "                        color = color_map[id_map[fly_idx]]\n",
        "                        (l,) = ax.plot(\n",
        "                            data_subset[(idx - 2) : idx, node_idx, 0, fly_idx]\n",
        "                            * scale_factor,\n",
        "                            data_subset[(idx - 2) : idx, node_idx, 1, fly_idx]\n",
        "                            * scale_factor,\n",
        "                            linewidth=3 * idx / data_subset.shape[0],\n",
        "                            color=color,\n",
        "                        )\n",
        "                        l.set_solid_capstyle(\"round\")\n",
        "        if video_path != None:\n",
        "            if cap.isOpened():\n",
        "                res, frame = cap.read()\n",
        "                frame = frame[:, :, 0]\n",
        "                plt.imshow(frame, cmap=\"gray\")\n",
        "        ax = plt.Axes(fig, [0.0, 0.0, 1.0, 1.0])\n",
        "        ax.set_axis_off()\n",
        "        fig.add_axes(ax)\n",
        "        fig.set_size_inches(3660 / dpi, 3660 / dpi, True)\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "        ax.axis(\"off\")\n",
        "        fig.patch.set_visible(False)\n",
        "        fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None)\n",
        "        fig.canvas.draw()\n",
        "        image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
        "        image_from_plot = image_from_plot.reshape(\n",
        "            fig.canvas.get_width_height()[::-1] + (3,)\n",
        "        )\n",
        "        ffmpeg_writer.writeFrame(image_from_plot)\n",
        "        plt.close()\n",
        "    ffmpeg_writer.close()\n",
        "\n",
        "def instance_node_velocities(fly_node_locations, start_frame, end_frame):\n",
        "    frame_count = len(range(start_frame, end_frame))\n",
        "    if len(fly_node_locations.shape) == 4:\n",
        "        fly_node_velocities = np.zeros(\n",
        "            (frame_count, fly_node_locations.shape[1], fly_node_locations.shape[3])\n",
        "        )\n",
        "        for fly_idx in range(fly_node_locations.shape[3]):\n",
        "            for n in tqdm(range(0, fly_node_locations.shape[1])):\n",
        "                fly_node_velocities[:, n, fly_idx] = diff(\n",
        "                    fly_node_locations[start_frame:end_frame, n, :, fly_idx]\n",
        "                )\n",
        "    else:\n",
        "        fly_node_velocities = np.zeros((frame_count, fly_node_locations.shape[1]))\n",
        "        for n in tqdm(range(0, fly_node_locations.shape[1] - 1)):\n",
        "            fly_node_velocities[:, n] = diff(\n",
        "                fly_node_locations[start_frame:end_frame, n, :]\n",
        "            )\n",
        "\n",
        "    return fly_node_velocities\n",
        "\n",
        "\n",
        "def diff(node_loc, diff_func = np.gradient, **kwargs):\n",
        "    \"\"\"\n",
        "    node_loc is a [frames, 2] arrayF\n",
        "\n",
        "    win defines the window to smooth over\n",
        "\n",
        "    poly defines the order of the polynomial\n",
        "    to fit with\n",
        "\n",
        "    \"\"\"\n",
        "    node_loc_vel = np.zeros_like(node_loc)\n",
        "    for c in range(node_loc.shape[-1]):\n",
        "        node_loc_vel[:, c] = diff_func(node_loc[:, c], **kwargs)\n",
        "\n",
        "    node_vel = np.linalg.norm(node_loc_vel,axis=1)\n",
        "\n",
        "    return node_vel\n",
        "\n",
        "def show_video(video_path, video_width = 1000):\n",
        "  video_file = open(video_path, \"r+b\").read()\n",
        "\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYaB5dugSx4b"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "px_mm = 15.5\n",
        "\n",
        "# Missingness filter\n",
        "atleast_one_node_defined = np.any(~np.isnan(locations[:, :, 0, :]), axis=1)\n",
        "no_nodes_defined =  ~atleast_one_node_defined\n",
        "\n",
        "missing_ct = np.sum(no_nodes_defined, axis=0)\n",
        "missing_freq = missing_ct / no_nodes_defined.shape[0]\n",
        "locations_filtered = locations[:, : , :, missing_freq < 0.8]\n",
        "\n",
        "\n",
        "vel = instance_node_velocities(locations_filtered,0,locations_filtered.shape[0])\n",
        "mask_2d = ~(vel[:,node_names.index('Thorax'),:] < px_mm*5 )[:,np.newaxis,np.newaxis,:]\n",
        "mask_4d = np.broadcast_to(mask_2d, locations_filtered.shape)\n",
        "locations_filtered[mask_4d] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.style\n",
        "import matplotlib as mpl\n",
        "from matplotlib.patches import Circle\n",
        "from matplotlib import patches\n",
        "import sleap\n",
        "\n",
        "mpl.rcParams[\"figure.facecolor\"] = \"w\"\n",
        "mpl.rcParams[\"figure.dpi\"] = 150\n",
        "mpl.rcParams[\"savefig.dpi\"] = 600\n",
        "mpl.rcParams[\"savefig.transparent\"] = True\n",
        "mpl.rcParams[\"font.size\"] = 15\n",
        "mpl.rcParams[\"font.family\"] = \"sans-serif\"\n",
        "mpl.rcParams[\"font.sans-serif\"] = [\"Arial\", \"DejaVu Sans\"]\n",
        "mpl.rcParams[\"axes.titlesize\"] = \"x-large\"  # medium, large, x-large, xx-large\n",
        "\n",
        "mpl.style.use(\"seaborn-deep\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dset = sleap.load_file(\"example_1h.slp\")\n",
        "track_occupancy = list(dset.get_track_occupancy(dset.video).values())\n",
        "track_start_end = [[rangelist.start, rangelist.end] for rangelist in track_occupancy]\n",
        "track_length =  [rangelist.end- rangelist.start for rangelist in track_occupancy]\n",
        "\n",
        "frame_rate = 20\n",
        "data=[ l/frame_rate for l in track_length],\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from turtle import title\n",
        "import h5py\n",
        "with h5py.File('example_1h.analysis.h5', 'r') as f:\n",
        "    occupancy_matrix_slp = f['track_occupancy'][:]\n",
        "    \n",
        "with h5py.File('/Genomics/ayroleslab2/scott/bees/naps_data/example_1h_1130to1230pm_naps.analysis.h5', 'r') as f:\n",
        "    occupancy_matrix_naps = f['track_occupancy'][:]\n",
        "    \n",
        "missing_freq_slp = np.sum(occupancy_matrix_slp.T == 1,axis=1) / occupancy_matrix_slp.shape[0]\n",
        "missing_freq_naps = np.sum(occupancy_matrix_naps.T == 1,axis=1) / occupancy_matrix_naps.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aruco_only =  np.load(\"output_aruco_1h.npy\")\n",
        "aruco_only = aruco_only[0:72000,0,0,:]\n",
        "occupancy_matrix_aruco = ~np.isnan(aruco_only)\n",
        "missing_freq_aruco = np.sum(occupancy_matrix_aruco.T == 1,axis=1) / occupancy_matrix_aruco.shape[0]\n",
        "missing_freq_aruco.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# palette = sns.color_palette(cc.glasbey, n_colors=arr.shape[3])\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
        "# fig.suptitle(\"Missingness frequency of tracklets\")\n",
        "sns.set_style(\"ticks\")\n",
        "\n",
        "bins = np.arange(0, 1.01, 0.05)\n",
        "sns.histplot(\n",
        "    ax=ax,\n",
        "    data=missing_freq_slp,\n",
        "    bins=bins,\n",
        "    color=palettable.wesanderson.Aquatic2_5.mpl_colors[0],\n",
        "    legend=False,\n",
        "    stat=\"density\",\n",
        "    alpha=0.6\n",
        ")\n",
        "\n",
        "sns.histplot(\n",
        "    ax=ax,\n",
        "    data=missing_freq_naps,\n",
        "    bins=bins,\n",
        "    color=palettable.wesanderson.Aquatic2_5.mpl_colors[1],\n",
        "    legend=False,\n",
        "    stat=\"density\",\n",
        "    alpha=0.6\n",
        ")\n",
        "sns.histplot(\n",
        "    ax=ax,\n",
        "    data=missing_freq_aruco,\n",
        "    bins=bins,\n",
        "    color=palettable.wesanderson.Aquatic2_5.mpl_colors[2],\n",
        "    legend=False,\n",
        "    stat=\"density\",\n",
        "    alpha=0.5\n",
        ")\n",
        "\n",
        "plt.xlim(0, 1)\n",
        "sns.despine(offset=3, trim=False)\n",
        "plt.legend(labels=[\"SLEAP\", \"NAPS\", \"ArUco\"], title = \"Source of Tracks\")\n",
        "\n",
        "plt.savefig(\"figures/track_length_distribution.png\", dpi=600)\n",
        "\n",
        "# TODO: Violin plots SLEAP, NAPS, ARUCO\n",
        "# Track lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mpl.rcParams[\"figure.facecolor\"] = \"w\"\n",
        "mpl.rcParams[\"figure.dpi\"] = 150\n",
        "mpl.rcParams[\"savefig.dpi\"] = 600\n",
        "mpl.rcParams[\"savefig.transparent\"] = True\n",
        "mpl.rcParams[\"font.size\"] = 15\n",
        "mpl.rcParams[\"font.family\"] = \"sans-serif\"\n",
        "mpl.rcParams[\"font.sans-serif\"] = [\"Arial\", \"DejaVu Sans\"]\n",
        "mpl.rcParams[\"axes.titlesize\"] = \"x-large\"  # medium, large, x-large, xx-large\n",
        "\n",
        "mpl.style.use(\"seaborn-deep\")\n",
        "# palette = sns.color_palette(cc.glasbey, n_colors=arr.shape[3])\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "# fig.suptitle(\"Missingness frequency of tracklets\")\n",
        "sns.set_style(\"ticks\")\n",
        "import itertools\n",
        "def patch_violinplot():\n",
        "     from matplotlib.collections import PolyCollection\n",
        "     ax = plt.gca()\n",
        "     for art in ax.get_children():\n",
        "          if isinstance(art, PolyCollection):\n",
        "              art.set_edgecolor('#424952')\n",
        "              art.set_linewidth(1)\n",
        "              art.set_alpha(0.5)\n",
        "              \n",
        "list_of_missing_freqs = [missing_freq_aruco, missing_freq_slp, missing_freq_naps]\n",
        "data = pd.DataFrame(\n",
        "    (_ for _ in itertools.zip_longest(*list_of_missing_freqs)),\n",
        "    columns=[\"ArUco\", \"SLEAP\", \"NAPS\"],\n",
        ")\n",
        "sns.violinplot(\n",
        "    ax=ax,\n",
        "    data=data * 100,\n",
        "    scale=\"width\",\n",
        "    inner=\"quartile\",\n",
        "    bw=0.15,\n",
        "    palette=palettable.wesanderson.Moonrise7_5.mpl_colors,\n",
        "    linewidth=1.5,\n",
        "    \n",
        ")\n",
        "patch_violinplot()\n",
        "plt.ylim(0, 100)\n",
        "sns.despine(offset=10, trim=False)\n",
        "plt.ylabel(\"Track Occupancy (%)\")\n",
        "plt.title(\"Track Occupancy Distribution by Source\", fontsize=16, y=1.025)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/track_length_violin.jpeg\", dpi=600)\n",
        "# plt.legend(labels=[\"SLEAP\", \"NAPS\", \"ArUco\"], title = \"Source of Tracks\", loc=\"upper center\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "naps_basic_workflow",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.11 ('sleap_dev')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "177c836934b6a63d57a075b5cc3f7812a3de8a98495a556647184ecb20fdf5fb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
